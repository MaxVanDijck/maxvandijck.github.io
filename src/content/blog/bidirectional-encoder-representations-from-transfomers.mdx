---
draft: true
title: "Bidirectional Encoder Representations from Transformers"
snippet: ""
publishDate: "2023-04-04 19:43"
image: {
  src: '/attention-is-all-you-need.png',
  alt: "BERT model"
}
category: "Deep Learning"
author: "Max van Dijck"
tags: [AI, datascience, deeplearning]
---

Bidirectional Encoder Representations from Transformers (BERT) is a transformer model shortly introduced after the publication of Attention is All You Need in 2017. 
Introduced in the paper is a model which can be applied to problems such as sentiment analysis, sentence pair classification tasks, question answering or named-entity recognition.
This blog post intends to summarise the architecture alongside PyTorch example implementations to offer an intuitive understanding of BERT.
We will also cover the unsupervised pre-training and fine-tuning methods in the paper for the various downstream tasks in relation to the architectural design. 
We hope that through these explanations, the application landscape of BERT is thoroughly demonstrated and that you, the reader, leave with a strong understanding of where BERT can be applied within your own practice. Throughout this blog post we assume knowledge of the transformer architecture and key details such as the attention mechanism & masking, tokenisation and embeddings. 

## Model Architecture

The BERT architecture consists of an embedding layer, an attention mask which both feed through to the primary component, the transformer encoder block. This encoder is the standard implementation using multi-head attention with the scaled dot-product attention mechanism. To help understand, we will walk through each individual component.

### Input Representation

BERT requires an input representation that integrates token, segment, and positional information. This is achieved through:

1. **Tokenization**: The tokenizer splits input text into subword units (e.g., WordPiece tokens). Each token is mapped to a dense vector from a pre-trained embedding matrix.

2. **Segment Embeddings**: These indicate whether a token belongs to sentence A or sentence B (used in tasks requiring sentence pair inputs, such as entailment or Q&A).

3. **Positional Embeddings**: To retain the order of the sequence, learnable positional encodings are added to the embeddings.

The sum of these three embeddings forms the final input representation that passes through the transformer encoder.

```python
class InputRepresentation(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, max_position_embeddings: int, type_vocab_size: int):
        super().__init__()
        self.token_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.position_embeddings = nn.Embedding(max_position_embeddings, embedding_dim)
        self.segment_embeddings = nn.Embedding(type_vocab_size, embedding_dim)
        self.layer_norm = nn.LayerNorm(embedding_dim)

    def forward(self, input_ids, token_type_ids, position_ids):
        token_embeds = self.token_embeddings(input_ids)
        position_embeds = self.position_embeddings(position_ids)
        segment_embeds = self.segment_embeddings(token_type_ids)
        embeddings = token_embeds + position_embeds + segment_embeds
        return self.layer_norm(embeddings)

vocab_size = 30522
embedding_dim = 768
max_position_embeddings = 512
type_vocab_size = 2

input_representation = InputRepresentation(vocab_size, embedding_dim, max_position_embeddings, type_vocab_size)
input_ids = torch.randint(0, vocab_size, (1, 10))
token_type_ids = torch.zeros((1, 10), dtype=torch.long)
position_ids = torch.arange(0, 10).unsqueeze(0)
embeddings = input_representation(input_ids, token_type_ids, position_ids)
embeddings.shape # torch.Size([1, 10, 768])
```

### Transformer Encoder Block
The encoder is a stack of identical layers, each consisting of:

1. **Multi-Head Self-Attention**: This mechanism allows each token to attend to every other token in the sequence, capturing contextual relationships. Scaled dot-product attention computes attention scores, which are normalized using softmax and combined to produce the output.

2. **Add & Norm Layers**: Following the attention operation, the output is added to the original input (residual connection) and passed through layer normalization.

3. **Feedforward Neural Network (FFN)**: A two-layer dense network with a GELU activation function processes the attention output. It projects the representation to a higher-dimensional space and back.

4. **Dropout**: Dropout is applied after attention and FFN to prevent overfitting.

```python
class TransformerEncoderLayer(nn.Module):
    def __init__(self, embedding_dim, num_heads, ff_hidden_dim, dropout_rate):
        super().__init__()
        self.multihead_attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate)
        self.feed_forward = nn.Sequential(
            nn.Linear(embedding_dim, ff_hidden_dim),
            nn.GELU(),
            nn.Linear(ff_hidden_dim, embedding_dim)
        )
        self.norm1 = nn.LayerNorm(embedding_dim)
        self.norm2 = nn.LayerNorm(embedding_dim)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x, attention_mask):
        # Multi-Head Self-Attention
        attn_output, _ = self.multihead_attention(x, x, x, attn_mask=attention_mask)
        x = self.norm1(x + self.dropout(attn_output))

        # Feed Forward
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        return x

# Example Usage
embedding_dim = 768
num_heads = 12
ff_hidden_dim = 3072
dropout_rate = 0.1
sequence_length = 10

encoder_layer = TransformerEncoderLayer(embedding_dim, num_heads, ff_hidden_dim, dropout_rate)
input_tensor = torch.rand(sequence_length, 1, embedding_dim)  # Sequence, Batch, Embedding
attention_mask = None
output = encoder_layer(input_tensor, attention_mask)
output.shape  # torch.Size([10, 1, 768])
```

### Attention Mask

BERT uses attention masks to distinguish real tokens from padding tokens in the input sequence. 
Padding tokens are ignored in computations to ensure meaningful attention scores. 
Unlike causal masking used in models like those described in "Attention is All You Need," where future tokens are masked to maintain unidirectional flow, BERT leverages bidirectionality. 
This means it can attend to all tokens in both directions, enabling it to capture more robust contextual relationships. 
Padding tokens, however, are excluded to avoid influencing the attention computations.

```python
# Example Attention Mask
attention_mask = torch.tensor([
    [1, 1, 1, 0, 0]  # 1 for real tokens, 0 for padding
])
attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # Expand for broadcasting
attention_mask = (1.0 - attention_mask) * -10000.0  # Convert to large negative value for masked tokens
```

### Output Layer

The output of the final encoder block is a sequence of contextualized embeddings, one for each input token. Depending on the task, specific elements of this output are used:

For single-sequence tasks (e.g., sentiment analysis), the [CLS] token embedding is used.

For sequence-pair tasks, embeddings for both [CLS] and token alignments are leveraged.

For token-level tasks (e.g., named-entity recognition), individual token embeddings are directly utilized.






